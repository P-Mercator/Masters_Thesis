#+TITLE: France regional electricity consumption clustering using Generalised Cross Correlation.
#+AUTHOR: Pierre Mercatoris
#+DATE: <2018-05-19 Sat>
#+EMAIL: mercatorispierre@gmail.com

#+PROPERTY: header-args    :eval no-export

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]
#+OPTIONS: toc:nil ^:{}
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_HEADER: \usepackage[top=1in, bottom=1.in, left=1in, right=1in]{geometry}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{fixltx2e}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{url}
#+LATEX_HEADER: \usepackage{minted}  % for source code
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{textcomp}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{pdfpages}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage[linktocpage, pdfstartview=FitH, colorlinks, linkcolor=blue, anchorcolor=blue, citecolor=blue,  filecolor=blue,  menucolor=blue,  urlcolor=blue]{hyperref}


* Introduction
** Cluster electricity consumption using GCC
   cite:jain1988algorithms
** Clustering time series
* Methodology
** Data description
   The electricity consumption was available at a 30 minutes frequency for each of
   the 12 regions of France from 2013 to 2017. Each year of each region can be
   downloaded from the French transmission operator (Rte) download portal[fn:1].

   Consumption from January 2013 to September of 2017 were downloaded for each of
   the 12 metropolitan mainland regions of France (excluding Corsica). 

   However, those regions are still very young, as before 2016, those were 21
   separate regions. Regions in France lack separate legislative power, but can
   manage a considerable part of their budget for main infrastructures such as
   education, public transport, universities and research, and help to businesses.
   It is therefore expected to find some interesting clusters, where we might see
   some reminiscence of the old regions.

[fn:1] http://www.rte-france.com/en/eco2mix/eco2mix-telechargement-en

** Data preparation
*** Cleaning

    The complete data set was spread across 60 different tables (years and regions) that were
    merged into one large table (table ref:tab:raw-series). 

#+caption: Original data structure. label:tab:raw-series
| Périmètre |       Date | Heures | Consommation |
|-----------+------------+--------+--------------|
| Grand-Est | 2016-01-01 |  00:00 |         5130 |
| Grand-Est | 2016-01-01 |  00:15 |              |
| Grand-Est | 2016-01-01 |  00:30 |         5130 |
| Grand-Est | 2016-01-01 |  00:45 |              |
| Grand-Est | 2016-01-01 |  01:00 |         5014 |
| .....     |            |        |              |

    As data rarely comes clean, there were some imperfections in the names of the
    regions. Some days the regions were named after the old ones e.g.
    Languedoc-Roussillon et Midi-Pyrénées instead of Occitanie, or Aquitaine,
    Limousin et Poitou-Charentes instead of Nouvelle-Aquitaine.

    With the raw data cleaned from imperfections, each column was formatted to
    required data type. A pivot table was then used so as to move each region as
    a column, and each row is a consumption measurement. The date then needed to
    be set as UTC in order to avoid problems at the summer/winter time change.
    As the original frequency of the data is 15 minutes but there are mostly
    only data every 30 minutes, the table was resampled by taking the sum for
    each 30 minutes, resulting in the table below (table
    ref:tab:regional-series).
 
#+BEGIN_SRC ipython :session :exports none :results silent
  from os.path import join
  import glob
  import pandas as pd

  data_path = "data"

  # Combine all the .xls interruptof each region
  data = pd.concat([
      pd.read_table(
          file, encoding="cp1252", delimiter="\t", engine="python",
          index_col=False).iloc[:-1, :]
      for file in glob.glob(join(data_path, "*.xls"))
  ])

  # Format type of variables
  data["Consommation"] = pd.to_numeric(data["Consommation"], errors='coerce')
  data["Datetime"] = pd.to_datetime(
      (data["Date"] + '_' + data["Heures"]).apply(str), format='%Y-%m-%d_%H:%M')

  # Correct regions names
  data.loc[data['Périmètre'] == 'Auvergne et Rhône-Alpes', 'Périmètre'] = 'Auvergne-Rhône-Alpes'
  data.loc[data['Périmètre'] == 'Bourgogne et Franche Comté', 'Périmètre'] = 'Bourgogne-Franche-Comté'
  data.loc[data['Périmètre'] == 'Alsace, Champagne-Ardenne et Lorraine', 'Périmètre'] = 'Grand-Est'
  data.loc[data['Périmètre'] == 'Nord-Pas-de-Calais et Picardie', 'Périmètre'] = 'Hauts-de-France'
  data.loc[data['Périmètre'] == 'Aquitaine, Limousin et Poitou-Charentes', 'Périmètre'] = 'Nouvelle-Aquitaine'
  data.loc[data['Périmètre'] == 'Languedoc-Roussillon et Midi-Pyrénées', 'Périmètre'] = 'Occitanie'

  # Reshape to row = datetime and column = region, all values are consumption
  consommation = pd.pivot_table(
      data, values='Consommation', index='Datetime', columns=['Périmètre'])
  # Set timezone as it creates problem when changing between daylight saving times.
  consommation = consommation.tz_localize('UTC', ambiguous=False)
  consommation = consommation.resample('30T').sum()
  #+END_SRC

#+caption: Regional series before splitting the series by time of the day. label:tab:regional-series
| Périmètre                 | Auvergne-Rhône-Alpes | Bourgogne-Franche-Comté | ... |
| Datetime                  |                      |                         |     |
|---------------------------+----------------------+-------------------------+-----|
| 2013-01-01_00:00:00+00:00 |                  NaN |                     NaN | ... |
| 2013-01-01_00:30:00+00:00 |               8173.0 |                  2357.0 | ... |
| 2013-01-01_01:00:00+00:00 |               7944.0 |                  2289.0 | ... |
| 2013-01-01_01:30:00+00:00 |               7896.0 |                  2326.0 |     |
| 2013-01-01_02:00:00+00:00 |               7882.0 |                  2409.0 |     |


The region with the highest consumption are observed in the Iles-de-France and
the lowest in the Centre-Val de Loire. We can also clearly see yearly
seasonality with higher consumption during winter times (figure ref:fig:regions-consumption).

   #+BEGIN_SRC ipython :session :ipyfile :exports results :results raw drawer
     import matplotlib.pyplot as plt
     %matplotlib inline

     consommation.loc[:,consommation.mean().sort_values(ascending=False).index].plot(
         alpha=0.7, lw=.1, figsize=(16,9), colormap='Spectral')
     leg = plt.legend(loc='upper right')
     for lh in leg.legendHandles:
         lh.set_linewidth(2)
         lh.set_alpha(1)
   #+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
#+caption: Mean electricity consumption of each of the french regions from 2013 to end 2017. label:fig:regions-consumption
[[file:./obipy-resources/3170sqC.png]]
:END:

The pivot table was used again so that each time of the day is a columns, and
each row is a daily value for a certain time and region, the resulting table has
576 columns (48 x 12 regions) and 1794 rows/days.(table ref:tab:final-data).

 #+BEGIN_SRC ipython :session :exports none :results silent
   import datetime

   consommation["date"] = pd.to_datetime(consommation.index).date
   consommation["time"] = pd.to_datetime(consommation.index).time
   consommation = pd.pivot_table(pd.melt(consommation, id_vars=["date", "time"]),
                               index="date", values="value", columns=["Périmètre", "time"])
   consommation = consommation.loc[datetime.date(2013,1,2):, :]
 #+END_SRC

#+caption: Final data format before export to csv. label:tab:final-data
|  Périmètre | Auvergne-Rhône-Alpes |          |          |
|       time |             00:00:00 | 00:30:00 | 01:00:00 |
|------------+----------------------+----------+----------|
|       date |                      |          |          |
| 2013-01-02 |               7847.0 |   7674.0 |   7427.0 |
| 2013-01-03 |               9028.0 |   8839.0 |   8544.0 |
| 2013-01-04 |               8982.0 |   8754.0 |   8476.0 |
| 2013-01-05 |               8625.0 |   8465.0 |   8165.0 |
| 2013-01-06 |               8314.0 |   8097.0 |   7814.0 |


In figure ref:fig:day-consumption, we can already see that consumption midday
is much higher than at night, with more spread in the summmer than in the winter.

   #+BEGIN_SRC ipython :session :ipyfile :exports results :results raw drawer
     mean_by_time  = consommation.groupby(level=1,  axis=1).mean().reset_index()
     mean_by_time.loc[:,mean_by_time.mean().sort_values(ascending=False).index].plot(
         alpha=0.9, lw=.5, figsize=(20,14), colormap='Spectral')
     leg = plt.legend(loc='upper right')
     for lh in leg.legendHandles:
         lh.set_linewidth(2)
         lh.set_alpha(1)
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[68]:
#+caption: Mean electricity consumption for all the regions of France at different times. label:fig:day-consumption
 [[file:./obipy-resources/3170jWw.png]]
 :END:

 #+BEGIN_SRC ipython :session :exports none :results silent
   # Merge multi index column names to read in R
   consommation.columns = [col[0] + '_' + str(col[1]) for col in consommation.columns.values]
   # Save to access from R
   consommation.to_csv(join(data_path, "consommation.csv"))
 #+END_SRC

*** Transformation

**** Stationarity
     
     The original series feature a strong seasonality as show in figure ref:fig:acf-raw.
 
 #+BEGIN_SRC R :session :exports results :results output graphics :file ./img/acf_diff7_R.png :width 900 :height 600
   library(tidyverse)
   library(xts)

   consommation <- read.csv('./data/consommation.csv', row.names='date')

   par(mfrow=c(3,4))
   par(mar=c(5.1,4.1,4.1,2.1))
   for (i in 1:12){
     acf(consommation[,(i-1)*48+1], lag=100, main=colnames(consommation)[(i-1)*48+1])
   }
 #+END_SRC

 #+RESULTS:
 #+caption: Autocorrelation function of the original data. label:fig:acf-raw
 [[file:./img/acf_diff7_R.png]]
 
To try and remove it, I have taken the weekly difference (difference between all
the values separated by 7 days). Now there is still some correlation, but it is
better (fig. ref:fig:acf-weekly).
 
 #+BEGIN_SRC R :session :exports results :results output graphics :file ./img/acf_test_R.png :width 900 :height 600
   par(mfrow=c(3,4))
   par(mar=c(5.1,4.1,4.1,2.1))
   for (i in 1:12){
     acf(diff(consommation[,(i-1)*48+1], 7), lag=100, main=colnames(consommation)[(i-1)*48+1])
   }
 #+END_SRC

 #+RESULTS:
 #+caption: Autocorrelation function of the weekly differenciated series. label:fig:acf-weekly
 [[file:./img/acf_test_R.png]]

So as to get as close to stationarity as possible without loosing too much data, I
have taken another difference, but this time only 1 day. Now, most of the values
stay within the confidence interval (fig. ref:fig:acf-final).

 #+BEGIN_SRC R :session :exports results :results output graphics :file ./img/acf_diff71_R.png :width 900 :height 600
   par(mfrow=c(3,4))
   par(mar=c(5.1,4.1,4.1,2.1))
   for (i in 1:12){
     acf(diff(diff(consommation[,(i-1)*48+1],7),1), lag=100, main=colnames(consommation)[(i-1)*48+1])
   }
 #+END_SRC

 #+RESULTS:
 #+caption: Autocorrelation function of the weekly differenciated series. label:fig:acf-final
 [[file:./img/acf_diff71_R.png]]
 

 I have then used the Dickey-Fuller test on all the series and confirmed that
 all the series are now significantly stationary (all p-values lower than 10e^{-21}).

 #+BEGIN_SRC R :session :exports none :results silent
   library(fpp)

   consommation <- diff(diff(as.matrix(consommation),7),1)
   max_p = 0
   for (i in 2:dim(consommation)[2]){
     p = adf.test(consommation[,i], alternative='stationary')$p.value
     if (p > max_p){
       max_p <- p
     }
   }
   print(paste(c('All values below', max_p), collapse=' '))
 #+END_SRC


 #+BEGIN_SRC ipython :session :ipyfile :exports none :results silent
   from statsmodels.tsa.stattools import acf
   import pandas as pd
   import matplotlib.pyplot as plt
   from os.path import join
   from statsmodels.tsa.stattools import adfuller
   %matplotlib inline

   data_path = "data"

   consommation = pd.read_csv(join(data_path, 'consommation.csv'), index_col=0)

   def test_stationarity(timeseries):
       # Perform Dickey-Fuller test:
       dftest = adfuller(timeseries, autolag="AIC")
       dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
       for key, value in dftest[4].items():
           dfoutput['Critical Value (%s)' % key] = value
       return dfoutput

   consommation = consommation.diff(7).diff(1).iloc[8:,:]
   p_values = consommation.apply(lambda x: test_stationarity(x)["p-value"])
   p_values.max()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[74]:
 : 1.601721472225731e-22
 :END:

**** Standardisation
     
 In order to standardise the data and get a mean of 0 and standard deviation of
 1, the z-score was applied to each individual series eqref:eq:zscore.

 \begin{equation}
 Z = \frac{x - \mu}{\sigma} \label{eq:zscore}
 \end{equation}
 
#+BEGIN_SRC R :session :exports none :results output drawer
  consommation <- scale(consommation)

  print(mean(consommation[,1]))
  print(sd(consommation[,1]))
#+END_SRC

#+RESULTS:
:RESULTS:
[1] -1.909605e-18
[1] 1
:END:
 

#+BEGIN_SRC ipython :session :exports none :results output drawer
  from scipy.stats.mstats import zscore
  consommation = consommation.apply(zscore, axis=0)
  print('Mean of z score is between', consommation.mean().min(), ' and ', consommation.mean().max())
  print('Std of z score is between', consommation.std().min(), ' and ', consommation.std().max())
#+END_SRC

#+RESULTS:
:RESULTS:
Mean of z score is between -3.4562374114870496e-17  and  4.674623261579606e-17
Std of z score is between 1.000280072824422  and  1.000280072824427
:END:
     
** GCC description
** Distance calculation
**** Selecting k
**** Distance matrix
* Results
** Clustering
** Cluster analysis
   
 #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/decompose_R.png
   library(tidyverse)
   library(xts)

   consommation <- read.csv('./data/consommation.csv', row.names='date')

   ts1 = ts(consommation[,1], frequency = 375, start = 2013)
   plot(decompose(ts1))
 #+END_SRC

 #+RESULTS:
 [[file:./img/decompose_R.png]]

* Conclusion


bibliographystyle:apalike
bibliography:ref.bib
