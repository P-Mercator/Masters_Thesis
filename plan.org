#+Author: Pierre Mercatoris
#+Title: Clustering large number of time series.

# -*- mode: org; -*-

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>


* Preparing the data


  #+BEGIN_SRC ipython :session
    import glob
    import inspect
    from os.path import join

    import itertools
    import pickle
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import scipy.cluster.hierarchy as hcl
    from scipy.spatial.distance import squareform
    from scipy.stats.mstats import zscore
    from statsmodels.tsa.stattools import adfuller, pacf, acf

    import src.helpers as helpers
  #+END_SRC

  I have downloaded all the regions definitive annual data from 2013 to 2015, the
  consolidated 2016 data as well as the running 2017 data. 


  #+BEGIN_SRC ipython :session
data_path = "data"

data = pd.concat([
    pd.read_table(
        file, encoding="iso8859_15", delimiter="\t", engine="python",
        index_col=False).iloc[1:-1, :]
    for file in glob.glob(join(data_path, "*.xls"))
])
  #+END_SRC


  #+BEGIN_SRC ipython :session
data = data.reset_index(drop=True)
data["Consommation"] = pd.to_numeric(data["Consommation"], errors='coerce')
data = data.loc[~data["Consommation"].isnull(), :]
data["Datetime"] = pd.to_datetime(
    (data["Date"] + '_' + data["Heures"]).apply(str), format='%Y-%m-%d_%H:%M')
data["Date"] = pd.to_datetime((data["Date"]).apply(str), format='%Y-%m-%d')
data["Heures"] = pd.to_datetime((data["Heures"]).apply(str), format='%H:%M',
                                infer_datetime_format=False).dt.time
  #+END_SRC




  #+BEGIN_SRC ipython :session
consommation = pd.pivot_table(
    data, values='Consommation', index='Datetime', columns=['Périmètre'])
consommation = consommation.drop('France', axis=1)
  #+END_SRC



** Quality control


   Removing all incomplete regions

  #+BEGIN_SRC ipython :session
  # Good series have more than 99% data
good_series = [
    consommation.iloc[:, i].isnull().sum() / consommation.iloc[:, i].shape[0] <
    0.99 for i in range(consommation.shape[1])
]
consommation = consommation.loc[:, good_series]
  #+END_SRC

  From around 2016, there are a lot of missing data. all records from that date
  were removed

#+BEGIN_SRC ipython :session :file  :exports both 
start_bad_date = np.unique(np.where(consommation.isnull())[0])[0]
bad_date = consommation.index[start_bad_date]
consommation = consommation.iloc[:start_bad_date-1, :]
#+END_SRC


#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
bad_date.date()
#+END_SRC

#+RESULTS:
:RESULTS:
: datetime.date(2016, 2, 29)
:END:


 
  #+BEGIN_SRC ipython :session :ipyfile ./img/plotSeries.png :exports both :results raw drawer
  %matplotlib inline
fig, ax = plt.subplots(4, 3, sharex=True, sharey=True)
i = 0
row = 0
for column in consommation.columns:
    col = i % 3
    consommation[column].plot(ax=ax[row, col])
    i += 1
    if col == 2:
        row += 1
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:./img/plotSeries.png]]
  :END:


    
  Transforming the table from regions series to daily/regions series for each hour

#+BEGIN_SRC ipython :session :file  :exports both
consommation["date"] = consommation.index.date
consommation["time"] = consommation.index.time
consommation = pd.pivot_table(pd.melt(consommation, id_vars=["date", "time"]), index="date", values="value",
                              columns=["Périmètre", "time"])
#+END_SRC

#+RESULTS:

There is an autocorrelation of 7 days (weekly)

#+BEGIN_SRC ipython :session :ipyfile ./img/7dayACF.png  :exports both :results raw drawer 
plt.figure()
ax = plt.gca()
for columns in consommation:
    plt.plot(acf(consommation.loc[:,columns].diff(7)[7:], nlags=100), alpha=0.05, color="black")
ax.set_xlabel("Lag")
ax.set_ylabel("Autocorrelation")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'Autocorrelation')
[[file:./img/7dayACF.png]]
:END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer
def test_stationarity(timeseries):
    # Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag="AIC")
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    return dfoutput

consommation = consommation.diff(7)[7:]
test_stationarity(consommation.iloc[:, 1])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  Test Statistic                -6.134467e+00
    p-value                        8.250744e-08
    #Lags Used                     2.200000e+01
    Number of Observations Used    1.124000e+03
    Critical Value (1%)           -3.436181e+00
    Critical Value (5%)           -2.864115e+00
    Critical Value (10%)          -2.568141e+00
    dtype: float64
  #+END_EXAMPLE
  :END:

Data was transformed using z-score and weekly differential  

  #+BEGIN_SRC ipython :session
consommation = consommation.apply(zscore, axis=0)
consommation = consommation.loc[:, consommation.isnull().sum() == 0]
consommation.index = pd.to_datetime(consommation.iloc[:, 0].index)
consommation = consommation.asfreq("1d")
  #+END_SRC

  #+RESULTS:



  #+BEGIN_SRC ipython :session :results raw drawer
  consommation.mean()[:5]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  Périmètre             time    
    Auvergne-Rhône-Alpes  00:30:00   -1.800362e-17
                          01:00:00    1.558378e-17
                          01:30:00   -1.621293e-17
                          02:00:00    3.194190e-17
                          02:30:00   -3.963699e-17
    dtype: float64
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :session :results raw drawer
  consommation.std()[:5]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  Périmètre             time    
    Auvergne-Rhône-Alpes  00:30:00    1.000436
                          01:00:00    1.000436
                          01:30:00    1.000436
                          02:00:00    1.000436
                          02:30:00    1.000436
    dtype: float64
  #+END_EXAMPLE
  :END:

* Calculation of GCC


   #+BEGIN_SRC ipython :session :results raw drawer
k = np.max([np.where(pacf(consommation.loc[:, colname]) < 0)[0][0] for colname, col in consommation.iteritems()])
k
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : 5
   :END:


  #+BEGIN_SRC ipython :session :results output code :exports both
print(inspect.getsource(helpers.k_matrix))
  #+END_SRC 

  #+RESULTS:
  #+BEGIN_SRC ipython
  def k_matrix(ts, k):
      return np.array([ts[(shift):ts.shape[0] - k + shift] for shift in np.arange(0, k + 1)]).T

  #+END_SRC


  #+BEGIN_SRC ipython :session :results output code :exports both
print(inspect.getsource(helpers.get_GCC))
  #+END_SRC 

  #+RESULTS:
  #+BEGIN_SRC ipython
  def get_GCC(ts1, ts2, k):
      Xi = k_matrix(ts1, k)
      Xj = k_matrix(ts2, k)
      Xij = np.concatenate((Xi, Xj), axis=1)
      GCC = 1 - np.linalg.det(np.corrcoef(Xij, rowvar=False) ** (1 / 2 * (k + 1))) / (
          np.linalg.det(np.corrcoef(Xi, rowvar=False) ** (1 / 2 * (k + 1))) \
          ,* np.linalg.det(np.corrcoef(Xj, rowvar=False) ** (1 / 2 * (k + 1))))
      return GCC

  #+END_SRC

  Then trying a few different ways of calculation the correlation matrix, I am
  struggling to get a correct value for GCC:

  #+BEGIN_SRC ipython :session :results raw drawer :exports both :eval no
DM_GCC = np.zeros((consommation.shape[1], consommation.shape[1]))
for i, j in itertools.combinations(range(consommation.shape[1]), 2):
    DM_GCC[i, j] = DM_GCC[j, i] = 1 - helpers.get_GCC(consommation.iloc[:, i], consommation.iloc[:, j], k)
DM_GCC = pd.DataFrame(DM_GCC, index=consommation.columns, columns=consommation.columns)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

#+BEGIN_SRC ipython :session :exports none :eval no
pickle.dump(DM_GCC, open(join(data_path, "DM_GCC.p"), "wb"))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports none
DM_GCC = pickle.load(open(join(data_path, "DM_GCC.p"), "rb"))
#+END_SRC



* Clustering

** Determination of the number of clusters
  #+BEGIN_SRC ipython :session :ipyfile  :exports both
linkage = hcl.linkage(squareform(DM_GCC), method="average")
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC ipython :session :ipyfile ./img/elbow.png :exports both :results raw drawer
  plt.figure()
  plt.plot(range(1, len(linkage)+1), linkage[::-1, 2])
  ax = plt.gca()
  ax.set_xlim([0,20])
  ax.set_ylim([0,1])
  ax.set_xlabel("Number of clusters")
  ax.set_ylabel("Between clusters distance")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'Between clusters distance')
[[file:./img/elbow.png]]
:END:


#+BEGIN_SRC ipython :session :exports both :results raw drawer
  elbow = np.diff(linkage[::-1, 2], 2)
  n_clust1 = elbow.argmax()+2
  elbow[elbow.argmax()] = 0
  n_clust2 = elbow.argmax()+2
  [n_clust1, n_clust2]
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

** Clustering methods comparison  

  #+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_TSNE.png :exports both :results raw drawer
    from sklearn.manifold import TSNE
    n_clusters = n_clust1
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")

    tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

    plt.figure()
    plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
    ax = plt.gca()
    ax.set_xlabel("x-tsne")
    ax.set_ylabel("y-tsne")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : Text(0,0.5,'y-tsne')
  [[file:./img/n_clust1_TSNE.png]]
  :END:



  #+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_TSNE.png :exports both :results raw drawer
    n_clusters = n_clust2
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")

    tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

    plt.figure()
    plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
    ax = plt.gca()
    ax.set_xlabel("x-tsne")
    ax.set_ylabel("y-tsne")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : Text(0,0.5,'y-tsne')
  [[file:./img/n_clust2_TSNE.png]]
  :END:



#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_spectral.png :exports both :results raw drawer
  from sklearn.cluster import SpectralClustering

  n_clusters = n_clust1
  clusters = SpectralClustering(n_clusters, affinity="precomputed").fit_predict(DM_GCC)

  tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

  plt.figure()
  plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
  ax = plt.gca()
  ax.set_xlabel("x-tsne")
  ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust1_spectral.png]]
:END:


#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_spectral.png :exports both :results raw drawer
  from sklearn.cluster import SpectralClustering

  n_clusters = n_clust2
  clusters = SpectralClustering(n_clusters, affinity="precomputed").fit_predict(DM_GCC)

  tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

  plt.figure()
  plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
  ax = plt.gca()
  ax.set_xlabel("x-tsne")
  ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust2_spectral.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_kmeans.png  :exports both :results raw drawer
from sklearn.cluster import KMeans

n_clusters = n_clust1
eigen_values, eigen_vectors = np.linalg.eigh(DM_GCC)
clusters = KMeans(n_clusters=n_clusters, init='k-means++').fit_predict(eigen_vectors[:, 2:4])

plt.figure()
plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
ax = plt.gca()
ax.set_xlabel("x-tsne")
ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust1_kmeans.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_kmeans.png  :exports both :results raw drawer
from sklearn.cluster import KMeans

n_clusters = n_clust2
eigen_values, eigen_vectors = np.linalg.eigh(DM_GCC)
clusters = KMeans(n_clusters=n_clusters, init='k-means++').fit_predict(eigen_vectors[:, 2:4])

plt.figure()
plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
ax = plt.gca()
ax.set_xlabel("x-tsne")
ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust2_kmeans.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_dbscan.png  :exports both :results raw drawer
    from sklearn.cluster import DBSCAN

    for eps in np.arange(0.0001, 0.01, 0.0001):
        clusters = DBSCAN(eps=eps, min_samples=10, metric="precomputed").fit_predict(DM_GCC)
        n_clusters = len(np.unique(clusters[clusters>0]))
        if n_clusters == n_clust1:
          plt.figure()
          plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
          ax = plt.gca()
          ax.set_xlabel("x-tsne")
          ax.set_ylabel("y-tsne")
          break
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust1_dbscan.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_dbscan.png  :exports both :results raw drawer
    from sklearn.cluster import DBSCAN

    for eps in np.arange(0.0001, 0.01, 0.0001):
        clusters = DBSCAN(eps=eps, min_samples=10, metric="precomputed").fit_predict(DM_GCC)
        n_clusters = len(np.unique(clusters[clusters>0]))
        if n_clusters == n_clust2:
          plt.figure()
          plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
          ax = plt.gca()
          ax.set_xlabel("x-tsne")
          ax.set_ylabel("y-tsne")
          break
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust2_dbscan.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/regions_dbscan.png  :exports both :results raw drawer
  from sklearn.cluster import DBSCAN

  for eps in np.arange(0.0001, 0.01, 0.0001):
      clusters = DBSCAN(eps=eps, min_samples=10, metric="precomputed").fit_predict(DM_GCC)
      n_clusters = len(np.unique(clusters[clusters>0]))
      if n_clusters == len(np.unique(consommation.columns.get_level_values("Périmètre"))):
          plt.figure()
          plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
          ax = plt.gca()
          ax.set_xlabel("x-tsne")
          ax.set_ylabel("y-tsne")
          break

#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./img/regions_dbscan.png]]
:END:


* Mapping the clusters

#+BEGIN_SRC ipython :session :exports both
    n_clusters = n_clust2
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")
    consommation_clusters = pd.DataFrame(np.transpose([[series[0] for series in consommation.columns.values],
                                                      [series[1] for series in consommation.columns.values],
                                                      list(clusters)]), columns=["Region", "Time", "Cluster"])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports both
  region_cluster = consommation_clusters.groupby(by="Region")["Cluster"].value_counts().index.to_frame()
  region_cluster.index = region_cluster["Region"].values

  region_codes = pd.read_csv("./data/frenchRegions.csv")

  region_cluster["Region"].isin(region_codes["Region"])
  region_cluster["region_match"] = region_cluster["Region"]

  region_codes = {}
  region_codes["Auvergne-Rhône-Alpes"] = [83, 82]
  region_codes["Bourgogne-Franche-Comté"] = [26, 43]
  region_codes["Bretagne"] = [53]
  region_codes["Centre-Val de Loire"] = [24]
  region_codes["Grand-Est"] = [42, 21, 41]
  region_codes["Hauts-de-France"] = [31, 22]
  region_codes["Ile-de-France"] = [11]
  region_codes["Normandie"] = [23, 25]
  region_codes["Nouvelle-Aquitaine"] = [72, 54, 74]
  region_codes["Occitanie"] = [91, 73]
  region_codes["PACA"] = [93]
  region_codes["Pays-de-la-Loire"] = [52]
#+END_SRC

#+RESULTS:


#+BEGIN_SRC ipython :session :exports code :results silent
import  pygal
from itertools import chain

fr_chart = pygal.maps.fr.Regions()
fr_chart.title = 'Regions clusters'
for cluster in np.unique(region_cluster["Cluster"]):
    fr_chart.add("Cluster " + str(cluster), 
                 list(chain.from_iterable([region_codes[region] 
                                           for region in region_cluster.loc[
                                               region_cluster["Cluster"]==cluster, "Region"].values])))
fr_chart.render_to_file("./img/regions_clusters.svg")
#+END_SRC

[[file:./img/regions_clusters.svg]]




* References:
   Ando, T. and Bai, J. (2016) Clustering huge number of financial time series: A panel data approach with high-dimensional predictors and factor structures. To appear at JASA. Available at: http://dx.doi.org/10.1080/01621459.2016.1195743
