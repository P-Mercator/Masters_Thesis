# -*- mode: org -*-

#+Author: Pierre Mercatoris
#+Title: Clustering large number of time series.
#+PROPERTY: header-args    :eval no-export


#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>


* Preparing the data

** Reading the data
*** In Python

  I have downloaded all the regions' annual data from 2013 to 2015, the
  consolidated 2016 data as well as the running 2017 data. The data came split
  between files for each 12 regions of France and each year, where each record
  contains the date, the time, the region and the electricity consumption.

  I have needed to correct the names of the regions as those where changed
  (probably by error) on the 29th of February 2016. 

[[http://www.rte-france.com/en/eco2mix/eco2mix-telechargement-en]]

#+BEGIN_SRC ipython :session :exports both :results raw drawer
  from os.path import join
  import glob
  import pandas as pd

  data_path = "data"

  # Combine all the .xls interruptof each region
  data = pd.concat([
      pd.read_table(
          file, encoding="cp1252", delimiter="\t", engine="python",
          index_col=False).iloc[:-1, :]
      for file in glob.glob(join(data_path, "*.xls"))
  ])

  # Format type of variables
  data["Consommation"] = pd.to_numeric(data["Consommation"], errors='coerce')
  data["Datetime"] = pd.to_datetime(
      (data["Date"] + '_' + data["Heures"]).apply(str), format='%Y-%m-%d_%H:%M')

  # Correct regions names
  data.loc[data['Périmètre'] == 'Auvergne et Rhône-Alpes', 'Périmètre'] = 'Auvergne-Rhône-Alpes'
  data.loc[data['Périmètre'] == 'Bourgogne et Franche Comté', 'Périmètre'] = 'Bourgogne-Franche-Comté'
  data.loc[data['Périmètre'] == 'Alsace, Champagne-Ardenne et Lorraine', 'Périmètre'] = 'Grand-Est'
  data.loc[data['Périmètre'] == 'Nord-Pas-de-Calais et Picardie', 'Périmètre'] = 'Hauts-de-France'
  data.loc[data['Périmètre'] == 'Aquitaine, Limousin et Poitou-Charentes', 'Périmètre'] = 'Nouvelle-Aquitaine'
  data.loc[data['Périmètre'] == 'Languedoc-Roussillon et Midi-Pyrénées', 'Périmètre'] = 'Occitanie'
 
  # Reshape to row = datetime and column = region, all values are consumption
  consommation = pd.pivot_table(
      data, values='Consommation', index='Datetime', columns=['Périmètre'])
  # Set timezone as it creates problem when changing between daylight saving times.
  consommation = consommation.tz_localize('UTC', ambiguous=False)
  consommation = consommation.resample('30T').mean()
  #+END_SRC
  

As all the regions are in the same columns, I have used a pivot table to get 1
column per region for each 'Datetime' (1 column for each of 12 regions). Additionally, it is important to set the timezone to UTC in order to account for
daylight saving time change and avoid removing data or introducing NAs.

In those 12 time series we can see some
outliers at the beginning of September 2017 where the data is close to 0. Those
gaps are expected as this data was not yet consolidated.

  #+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
    import matplotlib.pyplot as plt
    %matplotlib inline
    
    fig, ax = plt.subplots(4, 3, sharex=True, sharey=True)
    fig.set_size_inches(18,13)
    i = 0
    row = 0
    for column in consommation.columns:
        col = i % 3
        consommation[column].plot(ax=ax[row, col])
        i += 1
        if col == 2:
            row += 1
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[28]:
  [[file:./img/plotSeries.png]]
  :END:

  #+BEGIN_SRC ipython :session :ipyfile ./img/plotAllRegions.png :exports both :results raw drawer
    consommation.loc[:,consommation.mean().sort_values(ascending=False).index].plot(
        alpha=0.7, lw=.1, figsize=(16,9), colormap='Spectral')
    leg = plt.legend(loc='upper right')
    for lh in leg.legendHandles:
        lh.set_linewidth(2)
        lh.set_alpha(1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[184]:
  [[file:./img/plotAllRegions.png]]
  :END:
  
In order to make sure we are using
clean data, I have decided to use the records from the 2nd of January 2013 (1st
doesn't have data for midnight) to the 2nd of January 2017.

Furthermore, a second pivot table was used in order to create a column for each
30 minutes of the day. This resulted in a table composed of 576 daily time
series (48 for each of the 12 regions) over 1455 days.

#+BEGIN_SRC ipython :session :exports both :results raw drawer
  import datetime

  consommation["date"] = pd.to_datetime(consommation.index).date
  consommation["time"] = pd.to_datetime(consommation.index).time
  consommation = pd.pivot_table(pd.melt(consommation, id_vars=["date", "time"]),
                                index="date", values="value", columns=["Périmètre", "time"])

  # consommation = consommation.loc[datetime.date(2013,1,2):datetime.date(2017,1,2), :]
  consommation = consommation.loc[datetime.date(2013,1,2):, :]

  # Get rid of the 15 minutes columns (columns with nans)
  # consommation = consommation.loc[:,consommation.isnull().sum()!=consommation.shape[0]]

  consommation.head()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[95]:
#+BEGIN_EXAMPLE
  Périmètre  Auvergne-Rhône-Alpes                                               \
  time                   00:00:00 00:30:00 01:00:00 01:30:00 02:00:00 02:30:00
  date
  2013-01-02               7847.0   7674.0   7427.0   7441.0   7467.0   7550.0
  2013-01-03               9028.0   8839.0   8544.0   8560.0   8569.0   8667.0
  2013-01-04               8982.0   8754.0   8476.0   8480.0   8453.0   8554.0
  2013-01-05               8625.0   8465.0   8165.0   8134.0   8087.0   8149.0
  2013-01-06               8314.0   8097.0   7814.0   7791.0   7785.0   7842.0
  
  Périmètre                                        ...    Pays-de-la-Loire  \
  time       03:00:00 03:30:00 04:00:00 04:30:00   ...            19:00:00
  date                                             ...
  2013-01-02   7434.0   7371.0   7233.0   7311.0   ...              4336.0
  2013-01-03   8559.0   8483.0   8390.0   8392.0   ...              4279.0
  2013-01-04   8436.0   8386.0   8224.0   8195.0   ...              4181.0
  2013-01-05   7974.0   7897.0   7713.0   7597.0   ...              3877.0
  2013-01-06   7670.0   7605.0   7418.0   7352.0   ...              3854.0
  
  Périmètre                                                                  \
  time       19:30:00 20:00:00 20:30:00 21:00:00 21:30:00 22:00:00 22:30:00
  date
  2013-01-02   4228.0   4079.0   3923.0   3756.0   3565.0   3457.0   3510.0
  2013-01-03   4166.0   4038.0   3862.0   3712.0   3463.0   3308.0   3394.0
  2013-01-04   4123.0   3946.0   3755.0   3597.0   3559.0   3412.0   3456.0
  2013-01-05   3786.0   3696.0   3540.0   3449.0   3296.0   3221.0   3296.0
  2013-01-06   3834.0   3826.0   3771.0   3631.0   3494.0   3423.0   3420.0
  
  Périmètre
  time       23:00:00 23:30:00
  date
  2013-01-02   4003.0   3710.0
  2013-01-03   3909.0   3700.0
  2013-01-04   3903.0   3662.0
  2013-01-05   3864.0   3700.0
  2013-01-06   3942.0   3717.0
  
  [5 rows x 576 columns]
#+END_EXAMPLE
:END:

  #+BEGIN_SRC ipython :session :ipyfile ./img/plotAllTime.png :exports both :results raw drawer
    mean_by_time  = consommation.groupby(level=1,  axis=1).mean().reset_index()
    mean_by_time.loc[:,mean_by_time.mean().sort_values(ascending=False).index].plot(
        alpha=0.9, lw=.5, figsize=(20,14), colormap='Spectral')
    leg = plt.legend(loc='upper right')
    for lh in leg.legendHandles:
        lh.set_linewidth(2)
        lh.set_alpha(1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[190]:
[[file:./img/plotAllTime.png]]
:END:

With minimal data manipulation, I was able to format the data into 48 daily
series for each of the regions and get rid of all the 'missing' values.

#+BEGIN_SRC ipython :session :exports both :results output
print('Data dimensions: ', consommation.shape)
print('Number of NA values: ', consommation.isnull().sum().sum())
#+END_SRC

#+RESULTS:
: Data dimensions:  (1794, 576)
: Number of NA values:  0

As you can see, this gives us a matrix of 1794 rows (days) and 576 columns (48
half-hour of each 12 regions per day), with no NA values.

This data is now saved into a csv to read from R.

#+BEGIN_SRC ipython :session :exports both :results raw drawer
  # Merge multi index column names to read in R
  consommation.columns = [col[0] + '_' + str(col[1]) for col in consommation.columns.values]
  # Save to access from R
  consommation.to_csv(join(data_path, "consommation.csv"))
  # consommation = pd.read_csv(join(data_path, "consommation.csv"),index_col=[0], header=[0,1])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[192]:
:END:


** Transform the data
*** Stationarity


 As all the series are daily values there is a strong weekly seasonality within
 the raw values. Looking at the decomposition of one of the series, we can also
 clearly see the yearly seasonality.

 #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/decompose_R.png
   library(tidyverse)
   library(xts)


   consommation <- read.csv('./data/consommation.csv', row.names='date')

   ## consommation <- xts(consommation, order.by = as.Date(as.POSIXct(parse_date(rownames(consommation)))))

   ts1 = ts(consommation[,1], frequency = 375, start = 2013)
   plot(decompose(ts1))
 #+END_SRC

 #+RESULTS:
 [[file:./img/decompose_R.png]]

In order to transform the data to stationary series, we need to study the
 autocorrelation function. The black lines show the autocorrelation function until lag 100
 of each individual series, while the red one is the function of the mean of the
 series. That first autocorrelation clearly shows the weekly seasonality.
 #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/acf_diff7_R.png :width 900 :height 600
   par(mfrow=c(3,4))
   par(mar=c(5.1,4.1,4.1,2.1))
   for (i in 1:12){
     acf(diff(consommation[,(i-1)*48+1],7), lag=100, main=colnames(consommation)[(i-1)*48+1])
   }
 #+END_SRC

 #+RESULTS:
 [[file:./img/acf0_R.png]]

 #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/acf_diff71_R.png :width 900 :height 600
   par(mfrow=c(3,4))
   par(mar=c(5.1,4.1,4.1,2.1))
   for (i in 1:12){
     acf(diff(diff(consommation[,(i-1)*48+1],7),1), lag=100, main=colnames(consommation)[(i-1)*48+1])
   }
 #+END_SRC

 #+RESULTS:
 [[file:./img/acf_diff71_R.png]]
 

 #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/acf_test_R.png :width 900 :height 600
   par(mfrow=c(3,4))
   par(mar=c(5.1,4.1,4.1,2.1))
   for (i in 1:12){
     acf(consommation[,(i-1)*48+1], lag=100, main=colnames(consommation)[(i-1)*48+1])
   }
 #+END_SRC

 #+RESULTS:
 [[file:./img/acf_test_R.png]]

To try and remove it, I have taken the weekly difference (difference between all
the values separated by 7 days). Now there is still some correlation, but it is better.

So as to get as close stationarity as possible without loosing too much data, I
have taken another difference, but this time only 1 day. Now, most of the values
stay within the confidence interval.


 I have then used the Dickey-Fuller test on all the series and confirmed that
 all the series are now significantly stationary (all p-values lower than 0.01).

 #+BEGIN_SRC R :session :exports both :results output drawer
   library(fpp)

   consommation <- diff(diff(as.matrix(consommation),7),1)
   max_p = 0
   for (i in 2:dim(consommation)[2]){
     p = adf.test(consommation[,i], alternative='stationary')$p.value
     if (p > max_p){
       max_p <- p
     }
   }
   print(paste(c('All values below', max_p), collapse=' '))
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 Loading required package: forecast
 Loading required package: fma
 Loading required package: expsmooth
 Loading required package: lmtest
 Loading required package: tseries

     ‘tseries’ version: 0.10-44

     ‘tseries’ is a package for time series analysis and computational
     finance.

     See ‘library(help="tseries")’ for details.
 There were 50 or more warnings (use warnings() to see the first 50)
 [1] "All values below 0.01"
 :END:

 
In Python, I have done exactly the same thing and can see that we can the exact
same autocorrelation function.

 #+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
   from statsmodels.tsa.stattools import acf
   import pandas as pd
   import matplotlib.pyplot as plt
   from statsmodels.graphics.tsaplots import plot_acf 
   from os.path import join
   %matplotlib inline

   data_path = "data"

   consommation = pd.read_csv(join(data_path, 'consommation.csv'), index_col=0)
   consommation_diff = consommation.diff(7).iloc[7:,:]

   fig, axes = plt.subplots(3,4,sharex=True,sharey=True)
   fig.set_size_inches(16,12)
   for i,ax in zip(range(12),axes.reshape(-1)):
       plot_acf(consommation_diff.iloc[:,i*48+24], lags=50, ax=ax)
       ax.set_title(consommation.columns[i*48+24])
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[72]:
 [[file:./obipy-resources/3170wg2.png]]
 :END:

 #+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
   consommation_diff = consommation.diff(7).diff(1).iloc[8:,:]

   fig, axes = plt.subplots(3,4,sharex=True,sharey=True)
   fig.set_size_inches(16,12)
   for i,ax in zip(range(12),axes.reshape(-1)):
       plot_acf(consommation_diff.iloc[:,i*48+24], lags=50, ax=ax)
       ax.set_title(consommation.columns[i*48+24])
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[73]:
 [[file:./obipy-resources/3170iqF.png]]
 :END:
 
In Python is was possible to get the exact p-values and show that the largest
p-value is actually of order 10^{-22}.
 
   #+BEGIN_SRC ipython :session :exports both :results raw drawer
   from statsmodels.tsa.stattools import adfuller

 def test_stationarity(timeseries):
     # Perform Dickey-Fuller test:
     dftest = adfuller(timeseries, autolag="AIC")
     dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
     for key, value in dftest[4].items():
         dfoutput['Critical Value (%s)' % key] = value
     return dfoutput

 consommation = consommation.diff(7).diff(1).iloc[8:,:]
 p_values = consommation.apply(lambda x: test_stationarity(x)["p-value"])
 p_values.max()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[2]:
   : 1.3894735039903391e-08
   :END:
  
*** Data standardisation
   
 In order to standardise the data and get a mean of 0 and standard deviation of
 1, the z-score was applied to each individual series.

 $$Z = \frac{x - \mu}{\sigma}$$
 
#+BEGIN_SRC R :session :exports both :results output drawer
  consommation <- scale(consommation)

  print(mean(consommation[,1]))
  print(sd(consommation[,1]))
#+END_SRC

#+RESULTS:
:RESULTS:
[1] -2.064756e-17
[1] 1
:END:
 

#+BEGIN_SRC ipython :session :exports both :results output drawer
  from scipy.stats.mstats import zscore
  consommation = consommation.apply(zscore, axis=0)
  print('Mean of z score is between', consommation.mean().min(), ' and ', consommation.mean().max())
  print('Std of z score is between', consommation.std().min(), ' and ', consommation.std().max())
#+END_SRC

#+RESULTS:
:RESULTS:
Mean of z score is between -2.67646378852e-16  and  3.86683162018e-16
Std of z score is between 1.00027991603  and  1.00027991603
:END:

   
* Calculation of GCC

** Selection of k
*** PACF

    At first I decided to check for the order of AR from our times series by
    looking at the 'partial autocorrelation function', which is the autocorrelation
    of the series but controlling for the correlations between values at shorter
    lags.

    #+BEGIN_SRC ipython :session :ipyfile ./img/pacf_python.png  :exports both :results raw drawer
    from statsmodels.tsa.stattools import pacf    
    import numpy as np
    
    plt.figure()
    ax = plt.gca()
    all_pacf = np.array([pacf(consommation.loc[:,columns], nlags=100) for columns in consommation])
    mean_pacf = pacf(consommation.mean(axis=1).values, nlags=100)
    plt.axhline(1.96/np.sqrt(len(mean_pacf)), color='red')
    plt.axhline(-1.96/np.sqrt(len(mean_pacf)), color='red')
    for p in all_pacf:
    plt.plot(p, alpha=0.05, color="black")
    plt.plot(pacf(consommation.mean(axis=1), nlags=100), color='red')
    ax.set_xlabel("Lag")
    ax.set_ylabel("Partial Autocorrelation")
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[134]:
    : Text(0,0.5,'Partial Autocorrelation')
    [[file:./img/pacf_python.png]]
    :END:

    I then looked at those local minimums (for each 10 lag interval) by first getting the minimums of the mean
    pacf (red line).

    #+BEGIN_SRC ipython :session :results raw drawer :exports both
    mean_pacf = pacf(consommation.mean(axis=1).values, nlags=100)
    orders = [r[0] + mean_pacf[r[0] : r[1]].argmin()
    for r in zip(np.arange(0, 100, 10), np.arange(10, 110, 10))]
    orders
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[135]:
    : [7, 14, 21, 35, 42, 56, 63, 70, 84, 98]
    :END:

    And then I checked the minimum across all the series, so as to make sure that
    the order was large enough for all series. The values are the same originally
    but deviate at larger lag. It seems that 21 is the largest significant order.

    #+BEGIN_SRC ipython :session :results raw drawer :exports both
    orders = [r[0] + np.where(all_pacf[:, r[0]:r[1]] == all_pacf[:, r[0]:r[1]].min())[1][0]
    for r in zip(np.arange(0, 100, 10), np.arange(10, 110, 10))]
    orders
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[136]:
    : [7, 14, 21, 35, 42, 56, 63, 77, 84, 91]
    :END:

*** AR model fitting

**** python
    
     By fitting an AR model to each series with a maximum lag 40, python was able to
     get a k value of 37.

     #+BEGIN_SRC ipython :session :exports both :eval no
     import statsmodels.tsa.ar_model as ar
     k = consommation.apply(lambda x: ar.AR(x).select_order(maxlag=20, ic="bic", trend="nc")).max()
     k
     #+END_SRC

     #+RESULTS:
     : # Out[137]:
     : : 20

     #+BEGIN_SRC ipython :session :exports both 
       import statsmodels.api as sm
       k = consommation.apply(
           lambda x: sm.tsa.arma_order_select_ic(
               x, ic='bic', trend='nc', max_ar=40, max_ma=1)['bic_min_order'][0]).max()
       k
     #+END_SRC

**** R

     However, in R, k is 17.

     #+BEGIN_SRC R :session :exports both :results output drawer :eval no
     library(FitAR)

     getOrder <- function(ts, order.max=40) {
       SelectModel(ts, ARModel = 'AR', Criterion = 'BIC', lag.max = order.max)[1,1]
     }

     k <- max(apply(consommation, 2, getOrder))
     #+END_SRC

     #+BEGIN_SRC R :session :exports both :results output drawer 
     print(k)
     #+END_SRC

     #+RESULTS:
     :RESULTS:
     [1] 37
     :END:



** GCC

*** R

#+BEGIN_SRC R :session
k<-37
#+END_SRC

#+RESULTS:
: 37

#+BEGIN_SRC R :session
  kMatrix <- function(ts, k) {
    m <- ts[1 : (length(ts) - k)]
    for (i in seq(k)[2:k]) {
      m <- cbind(m, ts[i : (length(ts) - k + i - 1)])
    }
    m
  }

  GCC <- function(ts1, ts2, k) {
    Xi <-  kMatrix(ts1, k)
    Xj <-  kMatrix(ts2, k)

    Xij <- cbind(Xi, Xj)

    1 - det(cor(Xij))^(1/(2*(k+1))) /
      (det(cor(Xi))^(1/(2*(k+1))) * det(cor(Xj))^(1/(2*(k+1))))
  }

  combinations <- combn(dim(consommation)[2], 2)
  DM_GCC <- matrix(0, dim(consommation)[2], dim(consommation)[2])
  for (d in seq(dim(combinations)[2])) {
    distance <- GCC(consommation[, combinations[,d][1]],
                    consommation[, combinations[,d][2]], k)
    DM_GCC[combinations[,d][1], combinations[,d][2]] <- distance
    DM_GCC[combinations[,d][2], combinations[,d][1]] <- distance
  }
#+END_SRC
*** Python

    #+BEGIN_SRC ipython :session
    k=37
    #+END_SRC

    #+RESULTS:
    : # Out[4]:

    #+BEGIN_SRC ipython :session :results output code :exports both 
    import numpy as np
    from scipy.spatial.distance import pdist
    from scipy.spatial.distance import squareform
    import itertools
    import pickle


    def k_matrix(ts, k):
        T = ts.shape[0]
        return np.array(
            [ts[(shift):T - k + shift] for shift in np.arange(0, k + 1)])


    def get_GCC(ts1, ts2):
        k = 37
        Xi = k_matrix(ts1, k)
        Xj = k_matrix(ts2, k)
        Xij = np.concatenate((Xi, Xj))
        GCC = np.linalg.det(np.corrcoef(Xij)) ** (1 / (k + 1)) / (
            np.linalg.det(np.corrcoef(Xi)) ** (1 / (k + 1)) \
            ,* np.linalg.det(np.corrcoef(Xj)) ** (1 / (k + 1)) )
        return GCC


    pdist_gcc = pdist(consommation.values.T, get_GCC)
    DM_GCC = squareform(pdist_gcc)
    DM_GCC = pd.DataFrame(
        DM_GCC, index=consommation.columns, columns=consommation.columns)
    #+END_SRC 

    #+RESULTS:
    #+BEGIN_SRC ipython
    #+END_SRC



    #+BEGIN_SRC ipython :session :exports none 
    import pickle
    pickle.dump({'DM':DM_GCC,'pdist':pdist_gcc}, open(join(data_path, "GCC_37.p"), "wb"))
    #+END_SRC

    #+RESULTS:
    : # Out[14]:

    #+BEGIN_SRC ipython :session :exports none
    import pickle
    from os.path import join

    data_path = 'data'

    GCC = pickle.load(open(join(data_path, "GCC_37.p"), "rb"))
    DM_GCC = GCC['DM']
    pdist_gcc = GCC['pdist']
    #+END_SRC

    #+RESULTS:
    : # Out[8]:

* Clustering
** Linkage
*** Python
     #+BEGIN_SRC ipython :session :results output code :exports both 
       import scipy.cluster.hierarchy as hcl
       from scipy.spatial.distance import pdist

       linkage_gcc = hcl.single(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

       linkage_gcc = hcl.average(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

       linkage_gcc = hcl.centroid(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

       linkage_gcc = hcl.weighted(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

       linkage_gcc = hcl.median(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

       linkage_gcc = hcl.complete(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

       linkage_gcc = hcl.ward(pdist_gcc)
       print(hcl.cophenet(linkage_gcc, pdist_gcc)[0])

     #+END_SRC 

     #+RESULTS:
     #+BEGIN_SRC ipython
     0.699811597611
     0.753343991533
     0.743290414411
     0.764965857314
     0.701246468206
     0.688839271715
     0.70478579482
     #+END_SRC


     #+BEGIN_SRC ipython :session :ipyfile  :exports both
   import scipy.cluster.hierarchy as hcl
   from scipy.spatial.distance import squareform
   import numpy as np

   linkage = hcl.linkage(squareform(DM_GCC), method='weighted')
     #+END_SRC

     #+RESULTS:
     : # Out[95]:


   #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   import seaborn as sns
   linkage = hcl.linkage(squareform(DM_GCC), method='weighted')
   sns.clustermap(DM_GCC, row_linkage=linkage, col_linkage=linkage)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[228]:
   : <seaborn.matrix.ClusterGrid at 0x7fabc50167f0>
   [[file:./obipy-resources/117268hE.png]]
   :END:

   #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
     from scipy.cluster.hierarchy import dendrogram

     labels = [label.split('_')[0] for label in DM_GCC.columns.values]
     unique_labels = np.unique(labels)

     dendrogram(linkage,
                labels = labels)

     my_palette = plt.cm.get_cmap("nipy_spectral", len(unique_labels))
     label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}

     ax = plt.gca()
     xlbls = ax.get_xmajorticklabels()
     for lbl in xlbls:
         lbl.set_color(label_color[lbl.get_text()])

     ax.axhline(.5, color='red', linestyle='dashed')
     plt.show()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[177]:
   [[file:./obipy-resources/11726uBb.png]]
   :END:


    #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
      labels = [label.split('_')[1] for label in DM_GCC.columns.values]
      unique_labels = np.unique(labels)
      dendrogram(linkage,
                  labels = labels)
      my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
      label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
      ax = plt.gca()
      xlbls = ax.get_xmajorticklabels()
      for lbl in xlbls:
          lbl.set_color(label_color[lbl.get_text()])
      ax.axhline(.5, color='red', linestyle='dashed')
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[161]:
    : <matplotlib.lines.Line2D at 0x7fabc825b588>
    [[file:./obipy-resources/11726uzy.png]]
    :END:
*** R
    #+BEGIN_SRC R :session :results output drawer
    cor(as.dist(DM),cophenetic(hclust(as.dist(DM), method = 'average')))
    cor(as.dist(DM),cophenetic(hclust(as.dist(DM), method = 'single')))
    cor(as.dist(DM),cophenetic(hclust(as.dist(DM), method = 'median')))
    cor(as.dist(DM),cophenetic(hclust(as.dist(DM), method = 'ward.D2')))
    cor(as.dist(DM),cophenetic(hclust(as.dist(DM), method = 'complete')))
    cor(as.dist(DM),cophenetic(hclust(as.dist(DM), method = 'mcquitty')))
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    [1] 0.753344
    [1] 0.6998116
    [1] 0.5724388
    [1] 0.7047858
    [1] 0.6888393
    [1] 0.7649659
    :END:


    #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/dendrogram_R.png
      DM_GCC <- read.csv('./data/DM_GCC_37.csv', row.names=1)
      head(DM)[,1:5]
      hc <- hclust(as.dist(DM), method = 'mcquitty')
      plot(hc)
      groups <- cutree(hc, k=7)
      rect.hclust(hc, k=7, border="red")
    #+END_SRC

    #+RESULTS:
    [[file:./img/dendrogram_R.png]]

    #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/dendrogram2_R.png
    res <- hcut(as.dist(DM), k = 7, isdiss=TRUE)
    fviz_dend(res, rect = TRUE)
    #+END_SRC

    #+RESULTS:
    [[file:./img/dendrogram2_R.png]]

    #+BEGIN_SRC R :session :exports both :results output graphics :file ./img/region_clusters_R.png
    library(factoextra)
    fviz_cluster(list(data=DM, cluster=groups), geom='point')
    #+END_SRC

    #+RESULTS:
    [[file:./img/region_clusters_R.png]]

#+BEGIN_SRC R :session :exports both :results output graphics :file ./img/sil_clusters_R.png
require("cluster")
sil <- silhouette(groups, DM)
fviz_silhouette(sil)
#+END_SRC

#+RESULTS:
[[file:./img/sil_clusters_R.png]]

** Determination of the number of clusters
*** Python

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.figure()
  plt.plot(range(1, len(linkage)+1), linkage[::-1, 2])
  ax = plt.gca()
  ax.set_xlim([0,70])
  ax.set_ylim([0,1])
  ax.set_xlabel("Number of clusters")
  ax.set_ylabel("Between clusters distance")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[117]:
: Text(0,0.5,'Between clusters distance')
[[file:./obipy-resources/11726SkM.png]]
:END:


#+BEGIN_SRC ipython :session :exports both :results raw drawer
  elbow = np.diff(linkage[::-1, 2], 2)
  n_clust1 = elbow.argmax()+2
  elbow[elbow.argmax()] = 0
  n_clust2 = elbow.argmax()+2
  [n_clust1, n_clust2]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[118]:
: [4, 23]
:END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,70,1), [silhouette_score(DM_GCC,
                                                hcl.fcluster(linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,70,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[120]:
[[file:./obipy-resources/11726fuS.png]]
:END:


 #+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
# Compute the silhouette scores for each sample

n_clusters = 7
cluster_labels = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")
n_clusters = len(np.unique(cluster_labels))
sample_silhouette_values = silhouette_samples(DM_GCC, cluster_labels, metric='precomputed')
silhouette_avg = silhouette_score(DM_GCC, cluster_labels, metric='precomputed')

y_lower = 10
for i in range(n_clusters):
    # Aggregate the silhouette scores for samples belonging to
    # cluster i, and sort them
    ith_cluster_silhouette_values = \
        sample_silhouette_values[cluster_labels == i+1]

    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = cm.spectral(float(i) / n_clusters)
    plt.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    # Label the silhouette plots with their cluster numbers at the middle
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

    # Compute the new y_lower for next plot
    y_lower = y_upper + 10  # 10 for the 0 samples
    # The vertical line for average silhouette score of all the values
    plt.axvline(x=silhouette_avg, color="red", linestyle="--")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[175]:
[[file:./obipy-resources/11726UtO.png]]
:END:

 #+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
# Compute the silhouette scores for each sample

n_clusters = 8
cluster_labels = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")
n_clusters = len(np.unique(cluster_labels))
sample_silhouette_values = silhouette_samples(DM_GCC, cluster_labels, metric='precomputed')
silhouette_avg = silhouette_score(DM_GCC, cluster_labels, metric='precomputed')

y_lower = 10
for i in range(n_clusters):
    # Aggregate the silhouette scores for samples belonging to
    # cluster i, and sort them
    ith_cluster_silhouette_values = \
        sample_silhouette_values[cluster_labels == i+1]

    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = cm.spectral(float(i) / n_clusters)
    plt.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    # Label the silhouette plots with their cluster numbers at the middle
    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

    # Compute the new y_lower for next plot
    y_lower = y_upper + 10  # 10 for the 0 samples
    # The vertical line for average silhouette score of all the values
    plt.axvline(x=silhouette_avg, color="red", linestyle="--")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[176]:
[[file:./obipy-resources/11726h3U.png]]
:END:



** Clustering methods comparison  
*** Python

  #+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_TSNE.png :exports both :results raw drawer
    from sklearn.manifold import TSNE
    n_clusters = 7
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")

    tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

    plt.figure()
    plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
    ax = plt.gca()
    ax.set_xlabel("x-tsne")
    ax.set_ylabel("y-tsne")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[229]:
  : Text(0,0.5,'y-tsne')
  [[file:./img/n_clust1_TSNE.png]]
  :END:


  #+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  from sklearn.decomposition import PCA

  n_clusters = 7
  clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")

  pca_2dim = PCA(n_components=2).fit_transform(DM_GCC)

  plt.figure()
  plt.scatter(pca_2dim[:, 0], pca_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
  ax = plt.gca()
  ax.set_xlabel("1st component")
  ax.set_ylabel("2nd component")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[230]:
  : Text(0,0.5,'2nd component')
  [[file:./obipy-resources/117269Uj.png]]
  :END:

  


* Plot Clusters

** Mapping the clusters
*** Python

#+BEGIN_SRC ipython :session
  n_clusters = 7
  linkage = hcl.linkage(squareform(DM_GCC), method='weighted')
  clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")
#+END_SRC

#+RESULTS:
: # Out[231]:

 #+BEGIN_SRC ipython :session :exports both
   regions = [string.split('_')[0] for string in consommation.columns]
   times = [string.split('_')[1] for string in consommation.columns]
   consommation_clusters = pd.DataFrame(np.transpose([regions,
                                                   times,
                                                   list(clusters)]), columns=["Region", "Time", "Cluster"])
 #+END_SRC

 #+RESULTS:
 : # Out[232]:

 #+BEGIN_SRC ipython :session :exports both
   region_cluster = consommation_clusters.groupby(by="Region")["Cluster"].value_counts().index.to_frame()
   region_cluster.index = region_cluster["Region"].values

   region_codes = pd.read_csv("./data/frenchRegions.csv")

   region_cluster["Region"].isin(region_codes["Region"])
   region_cluster["region_match"] = region_cluster["Region"]

   region_codes = {}
   region_codes["Auvergne-Rhône-Alpes"] = [83, 82]
   region_codes["Bourgogne-Franche-Comté"] = [26, 43]
   region_codes["Bretagne"] = [53]
   region_codes["Centre-Val de Loire"] = [24]
   region_codes["Grand-Est"] = [42, 21, 41]
   region_codes["Hauts-de-France"] = [31, 22]
   region_codes["Ile-de-France"] = [11]
   region_codes["Normandie"] = [23, 25]
   region_codes["Nouvelle-Aquitaine"] = [72, 54, 74]
   region_codes["Occitanie"] = [91, 73]
   region_codes["PACA"] = [93]
   region_codes["Pays-de-la-Loire"] = [52]
 #+END_SRC

 #+RESULTS:
 : # Out[233]:


 #+BEGIN_SRC ipython :session :exports code :results silent
 import pygal
 from itertools import chain

 fr_chart = pygal.maps.fr.Regions()
 fr_chart.title = 'Regions clusters'
 for cluster in np.unique(region_cluster["Cluster"]):
     fr_chart.add("Cluster " + str(cluster), 
                  list(chain.from_iterable([region_codes[region] 
                                            for region in region_cluster.loc[
                                                region_cluster["Cluster"]==cluster, "Region"].values])))
 fr_chart.render_to_file("./img/regions_clusters.svg")
 #+END_SRC


* Check within regions clusters
** Python
  
 #+BEGIN_SRC ipython :session
   linkage = hcl.linkage(squareform(DM_GCC), method='weighted')
   clusters = hcl.fcluster(linkage, t=7, criterion="maxclust")
 #+END_SRC

 #+RESULTS:
 : # Out[235]:



 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==1, clusters==1]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[236]:
 [[file:./obipy-resources/11726Kfp.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[237]:
[[file:./obipy-resources/11726Xpv.png]]
:END:

 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==2, clusters==2]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[238]:
 [[file:./obipy-resources/11726kz1.png]]
 :END:


#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[239]:
[[file:./obipy-resources/11726W9E.png]]
:END:


 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==3, clusters==3]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[240]:
 [[file:./obipy-resources/11726jHL.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[241]:
[[file:./obipy-resources/11726wRR.png]]
:END:

 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==4, clusters==4]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[242]:
 [[file:./obipy-resources/117269bX.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[243]:
[[file:./obipy-resources/11726Kmd.png]]
:END:


 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==5, clusters==5]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   plt.subplot(211)
   labels = [label.split('_')[0] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("nipy_spectral", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.subplot(212)
   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[244]:
 [[file:./obipy-resources/11726Xwj.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[245]:
[[file:./obipy-resources/11726k6p.png]]
:END:


 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==6, clusters==6]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   plt.subplot(211)
   labels = [label.split('_')[0] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("nipy_spectral", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.subplot(212)
   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[246]:
 [[file:./obipy-resources/11726xEw.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[247]:
[[file:./obipy-resources/11726-O2.png]]
:END:

 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==7, clusters==7]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   plt.subplot(211)
   labels = [label.split('_')[0] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("nipy_spectral", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.subplot(212)
   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[249]:
 [[file:./obipy-resources/117269iL.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,10,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,10,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[250]:
[[file:./obipy-resources/11726KtR.png]]
:END:

 #+BEGIN_SRC ipython :session :ipyfile  :exports both :results raw drawer
   sub_DM_GCC = DM_GCC.loc[clusters==8, clusters==8]
   sub_linkage = hcl.linkage(squareform(sub_DM_GCC), method='weighted')

   plt.subplot(211)
   labels = [label.split('_')[0] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("nipy_spectral", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.subplot(212)
   labels = [label.split('_')[1] for label in sub_DM_GCC.columns.values]
   unique_labels = np.unique(labels)
   dendrogram(sub_linkage,
               labels = labels)
   my_palette = plt.cm.get_cmap("RdGy", len(unique_labels))
   label_color = {l:my_palette(i) for l, i in zip(unique_labels, np.arange(len(unique_labels)))}
   ax = plt.gca()
   xlbls = ax.get_xmajorticklabels()
   for lbl in xlbls:
       lbl.set_color(label_color[lbl.get_text()])

   plt.show()
 #+END_SRC

 #+RESULTS:
 :RESULTS:
 # Out[225]:
 [[file:./obipy-resources/11726wDp.png]]
 :END:

#+BEGIN_SRC ipython :session :ipyfile :exports both :results raw drawer
  plt.plot(np.arange(2,15,1), [silhouette_score(sub_DM_GCC,
                                                hcl.fcluster(sub_linkage, t=n, criterion="maxclust"),
                                                metric='precomputed')
                               for n in np.arange(2,15,1)])
  plt.show()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[227]:
[[file:./obipy-resources/11726KY1.png]]
:END:


* References:
   Ando, T. and Bai, J. (2016) Clustering huge number of financial time series: A panel data approach with high-dimensional predictors and factor structures. To appear at JASA. Available at: http://dx.doi.org/10.1080/01621459.2016.1195743
