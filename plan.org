# -*- mode: org -*-

#+Author: Pierre Mercatoris
#+Title: Clustering large number of time series.


#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>

#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>


* Preparing the data


  #+BEGIN_SRC ipython :session
  #+END_SRC

  #+RESULTS:

** Reading the data

  I have downloaded all the regions definitive annual data from 2013 to 2015, the
  consolidated 2016 data as well as the running 2017 data. 


 
#+BEGIN_SRC ipython :session :exports both :results raw drawer
    import glob
    import inspect
    from os.path import join

    import itertools
    import pickle
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import scipy.cluster.hierarchy as hcl
    from scipy.spatial.distance import squareform
    from scipy.stats.mstats import zscore
    from statsmodels.tsa.stattools import adfuller, pacf, acf
    import statsmodels.tsa.ar_model as ar
    import statsmodels.tsa.api as smt

    import src.helpers as helpers
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


#+BEGIN_SRC ipython :session :exports both :results raw drawer
  data_path = "data"

  # Combine all the .xls of each region
  data = pd.concat([
      pd.read_table(
          file, encoding="cp1252", delimiter="\t", engine="python",
          index_col=False).iloc[1:-1, :]
      for file in glob.glob(join(data_path, "*.xls"))
  ])

  data.to_csv(join(data_path, "all_raw.csv"), index_label=False)
  data = pd.read_csv(join(data_path, "all_raw.csv"), encoding="cp1252")

  # Format type of variables
  data = data.reset_index(drop=True)
  data["Consommation"] = pd.to_numeric(data["Consommation"], errors='coerce')
  data = data.loc[~data["Consommation"].isnull(), :]
  data["Datetime"] = pd.to_datetime(
      (data["Date"] + '_' + data["Heures"]).apply(str), format='%Y-%m-%d_%H:%M')
  data["Date"] = pd.to_datetime((data["Date"]).apply(str), format='%Y-%m-%d')
  data["Heures"] = pd.to_datetime((data["Heures"]).apply(str), format='%H:%M',
                                  infer_datetime_format=False).dt.time

  # Reshape to 48 series per region (1 daily series for each 30 mins)
  consommation = pd.pivot_table(
      data, values='Consommation', index='Datetime', columns=['Périmètre'])
  consommation = consommation.drop('France', axis=1)

  # Remove empty series
  good_series = [
      consommation.iloc[:, i].isnull().sum() / consommation.iloc[:, i].shape[0] <
      0.99 for i in range(consommation.shape[1])
  ]
  consommation = consommation.loc[:, good_series]

  # Save to access from R
  consommation.to_csv(join(data_path, "consommation.csv"), index_label="datetime")
  consommation = pd.read_csv(join(data_path, "consommation.csv"), encoding="cp1252", index_col="datetime")

  #+END_SRC

  #+RESULTS:





#+BEGIN_SRC R :session :exports both :results output drawer
  library(tidyverse)
  library(lubridate)

  data <- read.csv("data/all_raw.csv", row.names=NULL, encoding="cp1252")
  data$Date <- parse_date(data$Date)
  data$Heures <- parse_time(data$Heures)
  data$Consommation <- data$Consommation %>%
    as.character() %>%
    parse_double(na = c("", "NA", "-"))

  data <- data %>%
    select(c("Périmètre", "Consommation", "Date", "Heures"))%>%
    filter(Périmètre != "France")

  head(data[ ,-which(colMeans(is.na(spread(data, key=Périmètre, value = Consommation))) > 0.5)])

  goodRegions <- data %>%
    spread(key=Périmètre, value = Consommation) %>%
    is.na() %>%
    colMeans() < 0.5
  goodRegions <- names(which(goodRegions))
  data <- data[Périmètre %in% goodRegions, ]

  data.byHour <- data %>%
    spread(key=Heures, value=Consommation)

  colSums(!is.na(data.byHour))

  head(data)
#+END_SRC

#+RESULTS:
:RESULTS:
-- Attaching packages --------------------------------------- tidyverse 1.2.1 --
v ggplot2 2.2.1     v purrr   0.2.4
v tibble  1.3.4     v dplyr   0.7.4
v tidyr   0.7.2     v stringr 1.2.0
v readr   1.1.1     v forcats 0.2.0
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()

Attaching package: 'lubridate'

The following object is masked from 'package:base':

    date
:END:



** Reformat to regions series

  #+BEGIN_SRC ipython :session
consommation = pd.pivot_table(
    data, values='Consommation', index='Datetime', columns=['Périmètre'])
consommation = consommation.drop('France', axis=1)
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC R :session :results raw drawer :exports both

#+END_SRC


** Quality control


   Removing all incomplete regions

  #+BEGIN_SRC ipython :session
  # Good series have more than 99% data
good_series = [
    consommation.iloc[:, i].isnull().sum() / consommation.iloc[:, i].shape[0] <
    0.99 for i in range(consommation.shape[1])
]
consommation = consommation.loc[:, good_series]
  #+END_SRC

  #+RESULTS:

  From September 2017, there are a lot of missing data. all records from that date
  were removed

#+BEGIN_SRC ipython :session :exports both
np.unique(consommation.index[consommation.isnull().sum(axis=1)>0].date)
#+END_SRC

#+RESULTS:
: #+BEGIN_EXAMPLE
: array([datetime.date(2016, 2, 29), datetime.date(2017, 9, 3),
:          datetime.date(2017, 9, 10), datetime.date(2017, 9, 13),
:          datetime.date(2017, 9, 14), datetime.date(2017, 9, 16),
:          datetime.date(2017, 9, 17), datetime.date(2017, 9, 20),
:          datetime.date(2017, 9, 29), datetime.date(2017, 10, 1),
:          datetime.date(2017, 10, 3), datetime.date(2017, 10, 7),
:          datetime.date(2017, 10, 12), datetime.date(2017, 10, 13)], dtype=object)
: #+END_EXAMPLE


#+BEGIN_SRC ipython :session :ipyfile  :exports both
consommation = consommation.loc[:datetime.date(2017,9,1), :]
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :file  :exports both 
  # start_bad_date = np.unique(np.where(consommation.isnull())[0])[0]
  # bad_date = consommation.index[start_bad_date]
  # consommation = consommation.iloc[:start_bad_date-1, :]
#+END_SRC

#+RESULTS:


#+BEGIN_SRC ipython :session :file  :exports both :results raw drawer
  # bad_date.date()
#+END_SRC

#+RESULTS:
:RESULTS:
: datetime.date(2016, 2, 29)
:END:


 
  #+BEGIN_SRC ipython :session :ipyfile ./img/plotSeries.png :exports both :results raw drawer
  %matplotlib inline
fig, ax = plt.subplots(4, 3, sharex=True, sharey=True)
fig.set_size_inches(18,13)
i = 0
row = 0
for column in consommation.columns:
    col = i % 3
    consommation[column].plot(ax=ax[row, col])
    i += 1
    if col == 2:
        row += 1
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  [[file:./img/plotSeries.png]]
  :END:


    
  Transforming the table from regions series to daily/regions series for each hour

#+BEGIN_SRC ipython :session :file  :exports both
consommation["date"] = consommation.index.date
consommation["time"] = consommation.index.time
consommation = pd.pivot_table(pd.melt(consommation, id_vars=["date", "time"]), index="date", values="value",
                              columns=["Périmètre", "time"])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :ipyfile  :exports both
consommation = consommation.apply(lambda x: x.interpolate(limit_direction="both"))
#+END_SRC

#+RESULTS:


There is an autocorrelation of 7 days (weekly)

#+BEGIN_SRC ipython :session :ipyfile ./img/7dayACF.png  :exports both :results raw drawer 
plt.figure()
ax = plt.gca()
for columns in consommation:
    plt.plot(acf(consommation.loc[:,columns].diff(7)[7:], nlags=100), alpha=0.05, color="black")
ax.set_xlabel("Lag")
ax.set_ylabel("Autocorrelation")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'Autocorrelation')
[[file:./img/7dayACF.png]]
:END:

#+BEGIN_SRC ipython :session :exports both
consommation = consommation.diff(7)[7:]
#+END_SRC

#+RESULTS:


  #+BEGIN_SRC ipython :session :exports both :results raw drawer
def test_stationarity(timeseries):
    # Perform Dickey-Fuller test:
    dftest = adfuller(timeseries, autolag="AIC")
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    return dfoutput


  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

#+BEGIN_SRC ipython :session :ipyfile  :exports both
p_values = consommation.apply(lambda x: test_stationarity(x)["p-value"])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :ipyfile pValuesStationarity.png :exports both :results raw drawer
import seaborn as sns
sns.distplot(p_values.values, kde=False)
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x267b7e15438>
[[file:pValuesStationarity.png]]
:END:



Data was transformed using z-score and weekly differential  

  #+BEGIN_SRC ipython :session
consommation = consommation.apply(zscore, axis=0)
consommation.index = pd.to_datetime(consommation.iloc[:, 0].index)
consommation = consommation.asfreq("1d")
  #+END_SRC

  #+RESULTS:



  #+BEGIN_SRC ipython :session :results raw drawer
  consommation.mean()[:5]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  Périmètre             time    
    Auvergne-Rhône-Alpes  00:00:00    2.371572e-17
                          00:30:00    9.649847e-18
                          01:00:00   -7.752589e-18
                          01:30:00   -2.060815e-17
                          02:00:00   -2.064086e-17
    dtype: float64
  #+END_EXAMPLE
  :END:

  #+BEGIN_SRC ipython :session :results raw drawer
  consommation.std()[:5]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  #+BEGIN_EXAMPLE
  Périmètre             time    
    Auvergne-Rhône-Alpes  00:00:00    1.000295
                          00:30:00    1.000295
                          01:00:00    1.000295
                          01:30:00    1.000295
                          02:00:00    1.000295
    dtype: float64
  #+END_EXAMPLE
  :END:

* Calculation of GCC


   #+BEGIN_SRC ipython :session :results raw drawer
k = np.max([np.where(pacf(consommation.loc[:, colname]) < 0)[0][0] for colname, col in consommation.iteritems()])
k
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   : 5
   :END:
#+BEGIN_SRC ipython :session 
k = consommation.apply(lambda x: ar.AR(x).select_order(maxlag=30, ic="bic", trend="nc")).max()
k
#+END_SRC

#+RESULTS:
: : 38



  #+BEGIN_SRC ipython :session :results output code :exports both
print(inspect.getsource(helpers.k_matrix))
  #+END_SRC 

  #+RESULTS:
  #+BEGIN_SRC ipython
  def k_matrix(ts, k):
      return np.array([ts[(shift):ts.shape[0] - k + shift] for shift in np.arange(0, k + 1)]).T

  #+END_SRC


  #+BEGIN_SRC ipython :session :results output code :exports both
print(inspect.getsource(helpers.get_GCC))
  #+END_SRC 

  #+RESULTS:
  #+BEGIN_SRC ipython
  def get_GCC(ts1, ts2, k):
      Xi = k_matrix(ts1, k)
      Xj = k_matrix(ts2, k)
      Xij = np.concatenate((Xi, Xj), axis=1)
      GCC = 1 - np.linalg.det(np.corrcoef(Xij, rowvar=False) ** (1 / 2 * (k + 1))) / (
          np.linalg.det(np.corrcoef(Xi, rowvar=False) ** (1 / 2 * (k + 1))) \
          ,* np.linalg.det(np.corrcoef(Xj, rowvar=False) ** (1 / 2 * (k + 1))))
      return GCC

  #+END_SRC

  Then trying a few different ways of calculation the correlation matrix, I am
  struggling to get a correct value for GCC:

  #+BEGIN_SRC ipython :session :results raw drawer :exports both 
DM_GCC = np.zeros((consommation.shape[1], consommation.shape[1]))
for i, j in itertools.combinations(range(consommation.shape[1]), 2):
    DM_GCC[i, j] = DM_GCC[j, i] = 1 - helpers.get_GCC(consommation.iloc[:, i], consommation.iloc[:, j], k)
DM_GCC = pd.DataFrame(DM_GCC, index=consommation.columns, columns=consommation.columns)

pickle.dump(DM_GCC, open(join(data_path, "DM_GCC_38.p"), "wb"))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

#+BEGIN_SRC ipython :session :exports none :eval no
pickle.dump(DM_GCC, open(join(data_path, "DM_GCC.p"), "wb"))
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports none
DM_GCC = pickle.load(open(join(data_path, "DM_GCC.p"), "rb"))
#+END_SRC



* Clustering

** Determination of the number of clusters
  #+BEGIN_SRC ipython :session :ipyfile  :exports both
linkage = hcl.linkage(squareform(DM_GCC), method="average")
  #+END_SRC

  #+RESULTS:

#+BEGIN_SRC ipython :session :ipyfile ./img/elbow.png :exports both :results raw drawer
  plt.figure()
  plt.plot(range(1, len(linkage)+1), linkage[::-1, 2])
  ax = plt.gca()
  ax.set_xlim([0,20])
  ax.set_ylim([0,1])
  ax.set_xlabel("Number of clusters")
  ax.set_ylabel("Between clusters distance")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'Between clusters distance')
[[file:./img/elbow.png]]
:END:


#+BEGIN_SRC ipython :session :exports both :results raw drawer
  elbow = np.diff(linkage[::-1, 2], 2)
  n_clust1 = elbow.argmax()+2
  elbow[elbow.argmax()] = 0
  n_clust2 = elbow.argmax()+2
  [n_clust1, n_clust2]
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

** Clustering methods comparison  

  #+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_TSNE.png :exports both :results raw drawer
    from sklearn.manifold import TSNE
    n_clusters = n_clust1
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")

    tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

    plt.figure()
    plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
    ax = plt.gca()
    ax.set_xlabel("x-tsne")
    ax.set_ylabel("y-tsne")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : Text(0,0.5,'y-tsne')
  [[file:./img/n_clust1_TSNE.png]]
  :END:



  #+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_TSNE.png :exports both :results raw drawer
    n_clusters = n_clust2
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")

    tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

    plt.figure()
    plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
    ax = plt.gca()
    ax.set_xlabel("x-tsne")
    ax.set_ylabel("y-tsne")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : Text(0,0.5,'y-tsne')
  [[file:./img/n_clust2_TSNE.png]]
  :END:



#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_spectral.png :exports both :results raw drawer
  from sklearn.cluster import SpectralClustering

  n_clusters = n_clust1
  clusters = SpectralClustering(n_clusters, affinity="precomputed").fit_predict(DM_GCC)

  tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

  plt.figure()
  plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
  ax = plt.gca()
  ax.set_xlabel("x-tsne")
  ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust1_spectral.png]]
:END:


#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_spectral.png :exports both :results raw drawer
  from sklearn.cluster import SpectralClustering

  n_clusters = n_clust2
  clusters = SpectralClustering(n_clusters, affinity="precomputed").fit_predict(DM_GCC)

  tsne_2dim = TSNE(n_components=2, metric="precomputed").fit_transform(DM_GCC)

  plt.figure()
  plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
  ax = plt.gca()
  ax.set_xlabel("x-tsne")
  ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust2_spectral.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_kmeans.png  :exports both :results raw drawer
from sklearn.cluster import KMeans

n_clusters = n_clust1
eigen_values, eigen_vectors = np.linalg.eigh(DM_GCC)
clusters = KMeans(n_clusters=n_clusters, init='k-means++').fit_predict(eigen_vectors[:, 2:4])

plt.figure()
plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
ax = plt.gca()
ax.set_xlabel("x-tsne")
ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust1_kmeans.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_kmeans.png  :exports both :results raw drawer
from sklearn.cluster import KMeans

n_clusters = n_clust2
eigen_values, eigen_vectors = np.linalg.eigh(DM_GCC)
clusters = KMeans(n_clusters=n_clusters, init='k-means++').fit_predict(eigen_vectors[:, 2:4])

plt.figure()
plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
ax = plt.gca()
ax.set_xlabel("x-tsne")
ax.set_ylabel("y-tsne")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust2_kmeans.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust1_dbscan.png  :exports both :results raw drawer
    from sklearn.cluster import DBSCAN

    for eps in np.arange(0.0001, 0.01, 0.0001):
        clusters = DBSCAN(eps=eps, min_samples=10, metric="precomputed").fit_predict(DM_GCC)
        n_clusters = len(np.unique(clusters[clusters>0]))
        if n_clusters == n_clust1:
          plt.figure()
          plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
          ax = plt.gca()
          ax.set_xlabel("x-tsne")
          ax.set_ylabel("y-tsne")
          break
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust1_dbscan.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/n_clust2_dbscan.png  :exports both :results raw drawer
    from sklearn.cluster import DBSCAN

    for eps in np.arange(0.0001, 0.01, 0.0001):
        clusters = DBSCAN(eps=eps, min_samples=10, metric="precomputed").fit_predict(DM_GCC)
        n_clusters = len(np.unique(clusters[clusters>0]))
        if n_clusters == n_clust2:
          plt.figure()
          plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
          ax = plt.gca()
          ax.set_xlabel("x-tsne")
          ax.set_ylabel("y-tsne")
          break
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0,0.5,'y-tsne')
[[file:./img/n_clust2_dbscan.png]]
:END:

#+BEGIN_SRC ipython :session :ipyfile ./img/regions_dbscan.png  :exports both :results raw drawer
  from sklearn.cluster import DBSCAN

  for eps in np.arange(0.0001, 0.01, 0.0001):
      clusters = DBSCAN(eps=eps, min_samples=10, metric="precomputed").fit_predict(DM_GCC)
      n_clusters = len(np.unique(clusters[clusters>0]))
      if n_clusters == len(np.unique(consommation.columns.get_level_values("Périmètre"))):
          plt.figure()
          plt.scatter(tsne_2dim[:, 0], tsne_2dim[:, 1], c=clusters, cmap=plt.cm.get_cmap('Paired', n_clusters), alpha=0.3)
          ax = plt.gca()
          ax.set_xlabel("x-tsne")
          ax.set_ylabel("y-tsne")
          break

#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./img/regions_dbscan.png]]
:END:


* Mapping the clusters

#+BEGIN_SRC ipython :session :exports both
    n_clusters = n_clust2
    clusters = hcl.fcluster(linkage, t=n_clusters, criterion="maxclust")
    consommation_clusters = pd.DataFrame(np.transpose([[series[0] for series in consommation.columns.values],
                                                      [series[1] for series in consommation.columns.values],
                                                      list(clusters)]), columns=["Region", "Time", "Cluster"])
#+END_SRC

#+RESULTS:

#+BEGIN_SRC ipython :session :exports both
  region_cluster = consommation_clusters.groupby(by="Region")["Cluster"].value_counts().index.to_frame()
  region_cluster.index = region_cluster["Region"].values

  region_codes = pd.read_csv("./data/frenchRegions.csv")

  region_cluster["Region"].isin(region_codes["Region"])
  region_cluster["region_match"] = region_cluster["Region"]

  region_codes = {}
  region_codes["Auvergne-Rhône-Alpes"] = [83, 82]
  region_codes["Bourgogne-Franche-Comté"] = [26, 43]
  region_codes["Bretagne"] = [53]
  region_codes["Centre-Val de Loire"] = [24]
  region_codes["Grand-Est"] = [42, 21, 41]
  region_codes["Hauts-de-France"] = [31, 22]
  region_codes["Ile-de-France"] = [11]
  region_codes["Normandie"] = [23, 25]
  region_codes["Nouvelle-Aquitaine"] = [72, 54, 74]
  region_codes["Occitanie"] = [91, 73]
  region_codes["PACA"] = [93]
  region_codes["Pays-de-la-Loire"] = [52]
#+END_SRC

#+RESULTS:


#+BEGIN_SRC ipython :session :exports code :results silent
import  pygal
from itertools import chain

fr_chart = pygal.maps.fr.Regions()
fr_chart.title = 'Regions clusters'
for cluster in np.unique(region_cluster["Cluster"]):
    fr_chart.add("Cluster " + str(cluster), 
                 list(chain.from_iterable([region_codes[region] 
                                           for region in region_cluster.loc[
                                               region_cluster["Cluster"]==cluster, "Region"].values])))
fr_chart.render_to_file("./img/regions_clusters.svg")
#+END_SRC

[[file:./img/regions_clusters.svg]]

* Check within regions clusters


* References:
   Ando, T. and Bai, J. (2016) Clustering huge number of financial time series: A panel data approach with high-dimensional predictors and factor structures. To appear at JASA. Available at: http://dx.doi.org/10.1080/01621459.2016.1195743
